"""
Mean Reversion + Polynomial Derivative Backtest
================================================
Tests whether extended z-scores combined with velocity/acceleration
Exhaustion predicts forward returns.

Instruments: GC=F (Gold), NQ=F (Nasdaq), ES=F (S&P 500)
"""

import yfinance as yf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from itertools import product
from datetime import datetime, timedelta
import warnings

warnings.filterwarnings('ignore')


# =============================================================================
# DATA FETCHING
# =============================================================================

def fetch_data(tickers, period="60d", interval="5m"):
    """Fetch OHLCV data for multiple tickers."""
    data = {}
    for ticker in tickers:
        print(f"Fetching {ticker} ({interval} bars)...")
        df = yf.download(ticker, period=period, interval=interval, progress=False)
        if len(df) > 0:
            df.columns = df.columns.droplevel(1) if isinstance(df.columns, pd.MultiIndex) else df.columns
            data[ticker] = df
            print(f"  Got {len(df)} bars from {df.index[0]} to {df.index[-1]}")
        else:
            print(f"  WARNING: No data for {ticker}")
    return data


# =============================================================================
# INDICATOR CALCULATIONS
# =============================================================================

def calculate_ema(series, period):
    """Calculate EMA."""
    return series.ewm(span=period, adjust=False).mean()


def calculate_vwap_proxy(df, period=20):
    """
    Calculate a rolling VWAP-like measure for daily data.
    True VWAP resets daily, so for daily bars we use a volume-weighted moving average.
    """
    typical_price = (df['High'] + df['Low'] + df['Close']) / 3
    vwap = (typical_price * df['Volume']).rolling(period).sum() / df['Volume'].rolling(period).sum()
    return vwap


def calculate_adx(df, period=14):
    """Calculate ADX for regime filtering."""
    high = df['High']
    low = df['Low']
    close = df['Close']

    # True Range
    tr1 = high - low
    tr2 = abs(high - close.shift(1))
    tr3 = abs(low - close.shift(1))
    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
    atr = tr.rolling(period).mean()

    # Directional Movement
    up_move = high - high.shift(1)
    down_move = low.shift(1) - low

    plus_dm = np.where((up_move > down_move) & (up_move > 0), up_move, 0)
    minus_dm = np.where((down_move > up_move) & (down_move > 0), down_move, 0)

    plus_di = 100 * pd.Series(plus_dm, index=df.index).rolling(period).mean() / atr
    minus_di = 100 * pd.Series(minus_dm, index=df.index).rolling(period).mean() / atr

    dx = 100 * abs(plus_di - minus_di) / (plus_di + minus_di)
    adx = dx.rolling(period).mean()

    return adx


def calculate_zscore(price, mean, lookback=20):
    """Calculate z-score of price relative to mean."""
    deviation = price - mean
    std = deviation.rolling(lookback).std()
    zscore = deviation / std
    return zscore


def fit_polynomial(prices, window):
    """
    Fit quadratic polynomial to price over rolling window.
    Returns velocity (a1) and acceleration (a2) coefficients.

    P(t) = a0 + a1*t + a2*t^2
    Velocity = dP/dt = a1 + 2*a2*t (evaluated at end of window)
    Acceleration = d2P/dt2 = 2*a2
    """
    n = len(prices)
    velocity = np.full(n, np.nan)
    acceleration = np.full(n, np.nan)

    for i in range(window - 1, n):
        y = prices[i - window + 1:i + 1].values
        t = np.arange(window)

        # Fit quadratic: y = a0 + a1*t + a2*t^2
        coeffs = np.polyfit(t, y, 2)
        a2, a1, a0 = coeffs

        # Velocity at end of window (t = window - 1)
        velocity[i] = a1 + 2 * a2 * (window - 1)

        # Acceleration (constant for quadratic)
        acceleration[i] = 2 * a2

    return velocity, acceleration


def prepare_features(df, mean_type, mean_period, poly_window, zscore_lookback=20):
    """
    Prepare all features for signal generation.
    """
    price = df['Close']

    # Calculate mean based on type
    if mean_type == 'EMA20':
        mean = calculate_ema(price, 20)
    elif mean_type == 'EMA50':
        mean = calculate_ema(price, 50)
    elif mean_type == 'VWAP':
        mean = calculate_vwap_proxy(df, 20)
    else:
        raise ValueError(f"Unknown mean type: {mean_type}")

    # Z-score
    zscore = calculate_zscore(price, mean, zscore_lookback)

    # Polynomial fit for velocity and acceleration
    velocity, acceleration = fit_polynomial(price, poly_window)

    # ADX for regime filter
    adx = calculate_adx(df, 14)

    # Create features dataframe
    features = pd.DataFrame({
        'price': price,
        'mean': mean,
        'zscore': zscore,
        'velocity': velocity,
        'acceleration': acceleration,
        'adx': adx
    }, index=df.index)

    return features


# =============================================================================
# SIGNAL GENERATION
# =============================================================================

def generate_signals(features, zscore_thresh, vel_percentile, accel_percentile, adx_thresh=25):
    """
    Generate mean reversion signals.

    Long signal: z-score < -threshold AND acceleration > 0 (decelerating down)
    Short signal: z-score > +threshold AND acceleration < 0 (decelerating up)

    Additional filter: ADX < threshold (range day)
    """
    # Calculate velocity and acceleration thresholds from percentiles
    vel_abs = np.abs(features['velocity'].dropna())
    accel_abs = np.abs(features['acceleration'].dropna())

    vel_thresh = np.percentile(vel_abs, vel_percentile)
    accel_thresh = np.percentile(accel_abs, accel_percentile)

    signals = pd.Series(0, index=features.index)

    # Range day filter
    range_day = features['adx'] < adx_thresh

    # Long signals: extended below mean, decelerating (acceleration turning positive)
    long_condition = (
            (features['zscore'] < -zscore_thresh) &
            (features['acceleration'] > accel_thresh) &  # Acceleration opposing the move
            range_day
    )

    # Short signals: extended above mean, decelerating (acceleration turning negative)
    short_condition = (
            (features['zscore'] > zscore_thresh) &
            (features['acceleration'] < -accel_thresh) &  # Acceleration opposing the move
            range_day
    )

    signals[long_condition] = 1
    signals[short_condition] = -1

    return signals


# =============================================================================
# BACKTEST ENGINE
# =============================================================================

def calculate_forward_returns(df, holding_periods):
    """Calculate forward returns for multiple holding periods."""
    price = df['Close']
    forward_rets = {}

    for period in holding_periods:
        forward_rets[f'fwd_{period}'] = price.shift(-period) / price - 1

    return pd.DataFrame(forward_rets, index=df.index)


def analyze_signals(features, signals, forward_returns, holding_periods):
    """
    Analyze signal performance vs baseline.
    """
    results = []

    for period in holding_periods:
        col = f'fwd_{period}'

        # Get returns for each signal type
        long_signals = signals == 1
        short_signals = signals == -1

        long_rets = forward_returns.loc[long_signals, col].dropna()
        short_rets = -forward_returns.loc[short_signals, col].dropna()  # Flip sign for shorts

        all_signal_rets = pd.concat([long_rets, short_rets])

        # Baseline: unconditional returns
        baseline_rets = forward_returns[col].dropna()

        # Calculate metrics
        if len(all_signal_rets) > 0:
            results.append({
                'holding_period': period,
                'n_signals': len(all_signal_rets),
                'n_long': len(long_rets),
                'n_short': len(short_rets),
                'signal_mean': all_signal_rets.mean() * 100,
                'signal_std': all_signal_rets.std() * 100,
                'signal_sharpe': all_signal_rets.mean() / all_signal_rets.std() if all_signal_rets.std() > 0 else 0,
                'signal_win_rate': (all_signal_rets > 0).mean() * 100,
                'baseline_mean': baseline_rets.mean() * 100,
                'baseline_std': baseline_rets.std() * 100,
                'edge_vs_baseline': (all_signal_rets.mean() - baseline_rets.mean()) * 100,
                'long_mean': long_rets.mean() * 100 if len(long_rets) > 0 else np.nan,
                'short_mean': short_rets.mean() * 100 if len(short_rets) > 0 else np.nan,
            })
        else:
            results.append({
                'holding_period': period,
                'n_signals': 0,
                'n_long': 0,
                'n_short': 0,
                'signal_mean': np.nan,
                'signal_std': np.nan,
                'signal_sharpe': np.nan,
                'signal_win_rate': np.nan,
                'baseline_mean': baseline_rets.mean() * 100,
                'baseline_std': baseline_rets.std() * 100,
                'edge_vs_baseline': np.nan,
                'long_mean': np.nan,
                'short_mean': np.nan,
            })

    return pd.DataFrame(results)


def run_parameter_sweep(df, mean_types, zscore_thresholds, poly_windows,
                        vel_percentiles, accel_percentiles, holding_periods, adx_thresh=25):
    """
    Run backtest across all parameter combinations.
    """
    all_results = []

    # Pre-calculate forward returns (constant across parameter sets)
    forward_returns = calculate_forward_returns(df, holding_periods)

    total_combos = len(mean_types) * len(zscore_thresholds) * len(poly_windows) * len(vel_percentiles) * len(
        accel_percentiles)
    print(f"\nRunning {total_combos} parameter combinations...")

    combo_count = 0
    for mean_type in mean_types:
        for poly_window in poly_windows:
            # Calculate features once per mean_type/poly_window combo
            features = prepare_features(df, mean_type, None, poly_window)

            for zscore_thresh in zscore_thresholds:
                for vel_pct in vel_percentiles:
                    for accel_pct in accel_percentiles:
                        combo_count += 1

                        # Generate signals
                        signals = generate_signals(
                            features, zscore_thresh, vel_pct, accel_pct, adx_thresh
                        )

                        # Analyze performance
                        perf = analyze_signals(features, signals, forward_returns, holding_periods)

                        # Add parameters to results
                        perf['mean_type'] = mean_type
                        perf['zscore_thresh'] = zscore_thresh
                        perf['poly_window'] = poly_window
                        perf['vel_percentile'] = vel_pct
                        perf['accel_percentile'] = accel_pct

                        all_results.append(perf)

    print(f"Completed {combo_count} combinations")
    return pd.concat(all_results, ignore_index=True)


# =============================================================================
# REPORTING
# =============================================================================

def summarize_results(results_df, min_signals=30):
    """
    Summarize results and find robust parameter regions.
    """
    # Filter for sufficient sample size
    valid = results_df[results_df['n_signals'] >= min_signals].copy()

    if len(valid) == 0:
        print("WARNING: No parameter combinations had sufficient signals!")
        return None

    # Find best performers by edge
    summary = valid.groupby(['mean_type', 'holding_period']).agg({
        'signal_mean': ['mean', 'std', 'max'],
        'edge_vs_baseline': ['mean', 'std', 'max'],
        'signal_win_rate': ['mean', 'max'],
        'n_signals': 'mean',
        'signal_sharpe': ['mean', 'max']
    }).round(3)

    return summary


def plot_results(results_df, ticker, save_path=None):
    """
    Create visualization of backtest results.
    """
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    fig.suptitle(f'Mean Reversion Backtest Results: {ticker}', fontsize=14, fontweight='bold')

    valid = results_df[results_df['n_signals'] >= 20].copy()

    if len(valid) == 0:
        print("Not enough valid results to plot")
        return

    # 1. Edge vs Baseline by Mean Type and Holding Period
    ax1 = axes[0, 0]
    pivot = valid.groupby(['mean_type', 'holding_period'])['edge_vs_baseline'].mean().unstack()
    pivot.plot(kind='bar', ax=ax1, width=0.8)
    ax1.set_title('Average Edge vs Baseline (%)')
    ax1.set_xlabel('Mean Type')
    ax1.set_ylabel('Edge (%)')
    ax1.axhline(y=0, color='red', linestyle='--', alpha=0.5)
    ax1.legend(title='Hold Period')
    ax1.tick_params(axis='x', rotation=0)

    # 2. Win Rate Distribution
    ax2 = axes[0, 1]
    for mean_type in valid['mean_type'].unique():
        subset = valid[valid['mean_type'] == mean_type]['signal_win_rate']
        ax2.hist(subset, bins=20, alpha=0.5, label=mean_type)
    ax2.axvline(x=50, color='red', linestyle='--', label='50% baseline')
    ax2.set_title('Win Rate Distribution Across Parameters')
    ax2.set_xlabel('Win Rate (%)')
    ax2.set_ylabel('Count')
    ax2.legend()

    # 3. Signal Count vs Edge
    ax3 = axes[1, 0]
    scatter = ax3.scatter(valid['n_signals'], valid['edge_vs_baseline'],
                          c=valid['holding_period'], cmap='viridis', alpha=0.5)
    ax3.axhline(y=0, color='red', linestyle='--', alpha=0.5)
    ax3.set_title('Signal Count vs Edge')
    ax3.set_xlabel('Number of Signals')
    ax3.set_ylabel('Edge vs Baseline (%)')
    plt.colorbar(scatter, ax=ax3, label='Hold Period')

    # 4. Heatmap: Z-Score Threshold vs Poly Window (for best mean type)
    ax4 = axes[1, 1]
    best_mean = valid.groupby('mean_type')['edge_vs_baseline'].mean().idxmax()
    subset = valid[(valid['mean_type'] == best_mean) & (valid['holding_period'] == 20)]

    if len(subset) > 0:
        pivot = subset.groupby(['zscore_thresh', 'poly_window'])['edge_vs_baseline'].mean().unstack()
        sns.heatmap(pivot, annot=True, fmt='.2f', cmap='RdYlGn', center=0, ax=ax4)
        ax4.set_title(f'Edge Heatmap: {best_mean}, 20-bar hold')
        ax4.set_xlabel('Poly Window')
        ax4.set_ylabel('Z-Score Threshold')

    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"Saved plot to {save_path}")

    plt.show()
    return fig


def print_top_parameters(results_df, n=10, min_signals=30):
    """Print the top performing parameter combinations."""
    valid = results_df[results_df['n_signals'] >= min_signals].copy()

    if len(valid) == 0:
        print("No valid results with sufficient signals")
        return

    # Sort by edge
    top = valid.nlargest(n, 'edge_vs_baseline')

    print("\n" + "=" * 80)
    print("TOP PARAMETER COMBINATIONS BY EDGE VS BASELINE")
    print("=" * 80)

    cols = ['mean_type', 'zscore_thresh', 'poly_window', 'holding_period',
            'n_signals', 'signal_mean', 'edge_vs_baseline', 'signal_win_rate', 'signal_sharpe']

    print(top[cols].to_string(index=False))


def print_robustness_check(results_df, min_signals=30):
    """Check if edge is robust across parameter ranges."""
    valid = results_df[results_df['n_signals'] >= min_signals].copy()

    if len(valid) == 0:
        print("No valid results for robustness check")
        return

    print("\n" + "=" * 80)
    print("ROBUSTNESS CHECK: % of parameter combos with positive edge")
    print("=" * 80)

    for mean_type in valid['mean_type'].unique():
        subset = valid[valid['mean_type'] == mean_type]
        pct_positive = (subset['edge_vs_baseline'] > 0).mean() * 100
        avg_edge = subset['edge_vs_baseline'].mean()
        print(f"{mean_type}: {pct_positive:.1f}% positive edge, avg edge = {avg_edge:.3f}%")

    print("\nBy Holding Period:")
    for hp in sorted(valid['holding_period'].unique()):
        subset = valid[valid['holding_period'] == hp]
        pct_positive = (subset['edge_vs_baseline'] > 0).mean() * 100
        avg_edge = subset['edge_vs_baseline'].mean()
        print(f"  {hp} bars: {pct_positive:.1f}% positive edge, avg edge = {avg_edge:.3f}%")


# =============================================================================
# MAIN EXECUTION
# =============================================================================

def main():
    # Configuration for 5-minute intraday
    TICKERS = ['GC=F', 'NQ=F', 'ES=F']
    MEAN_TYPES = ['EMA20', 'EMA50', 'VWAP']
    ZSCORE_THRESHOLDS = [1.5, 2.0, 2.5, 3.0]
    POLY_WINDOWS = [10, 15, 20, 25, 30]
    VEL_PERCENTILES = [25, 50, 75]
    ACCEL_PERCENTILES = [25, 50, 75]
    HOLDING_PERIODS = [6, 12, 24]  # 30min, 1hr, 2hr on 5m bars
    ADX_THRESHOLD = 25

    # Fetch data
    print("=" * 80)
    print("MEAN REVERSION + POLYNOMIAL DERIVATIVE BACKTEST (5-MIN BARS)")
    print("=" * 80)

    data = fetch_data(TICKERS, period="60d", interval="5m")

    if len(data) == 0:
        print("ERROR: No data fetched. Check your internet connection.")
        return None

    # Check for volume data (needed for VWAP)
    for ticker, df in data.items():
        if 'Volume' in df.columns and df['Volume'].sum() > 0:
            print(f"{ticker}: Volume data available")
        else:
            print(f"{ticker}: No volume data - VWAP will be excluded")

    # Run backtest for each ticker
    all_ticker_results = {}

    for ticker, df in data.items():
        print(f"\n{'=' * 80}")
        print(f"BACKTESTING: {ticker}")
        print(f"{'=' * 80}")

        # Check if VWAP is possible
        mean_types = MEAN_TYPES.copy()
        if 'Volume' not in df.columns or df['Volume'].sum() == 0:
            mean_types = [m for m in mean_types if m != 'VWAP']
            print("Excluding VWAP due to missing volume data")

        # Run parameter sweep
        results = run_parameter_sweep(
            df,
            mean_types=mean_types,
            zscore_thresholds=ZSCORE_THRESHOLDS,
            poly_windows=POLY_WINDOWS,
            vel_percentiles=VEL_PERCENTILES,
            accel_percentiles=ACCEL_PERCENTILES,
            holding_periods=HOLDING_PERIODS,
            adx_thresh=ADX_THRESHOLD
        )

        all_ticker_results[ticker] = results

        # Print results
        print_top_parameters(results, n=10, min_signals=30)
        print_robustness_check(results, min_signals=30)

        # Create plots
        plot_results(results, ticker, save_path=f'{ticker.replace("=", "_")}_backtest.png')

    # Save detailed results to CSV
    for ticker, results in all_ticker_results.items():
        csv_path = f'{ticker.replace("=", "_")}_results.csv'
        results.to_csv(csv_path, index=False)
        print(f"\nSaved detailed results to {csv_path}")

    print("\n" + "=" * 80)
    print("BACKTEST COMPLETE")
    print("=" * 80)

    return all_ticker_results


if __name__ == "__main__":
    results = main()
